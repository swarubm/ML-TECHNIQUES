# SPECIALIZED ML-TECHNIQUES
CROSS VALIDATION .ipynb: Demonstrates four major cross-validation methods using the Iris dataset. Helps evaluate and compare model performance by partitioning data into training and testing segments, a core ML validation technique.

ROC-AUC curve.ipynb: Shows how to compute and plot ROC-AUC curves, which are used for judging binary classification model quality. This notebook includes code and practical outputs for interpreting classifier results.

encoding categorical data.ipynb: Provides a guide to encoding categorical variables using One-Hot Encoding and Label Encoding—essential for converting non-numeric data into ML-friendly formats.

imputation.ipynb: Covers various data imputation techniques as part of EDA (Exploratory Data Analysis), focusing on how to handle and fill missing values in datasets before modeling.

log transformation.ipynb: Teaches and demonstrates log transformation to convert skewed data distributions into more normal forms, improving model suitability and performance.

model_selection_and_comparing_with_classifiers.ipynb: Explores how to select models and compare different classifiers in practice. Includes code for fitting and evaluating various ML classifiers, possibly with comparative metrics and result analysis.

README.md: The readme likely offers a summary or overview, helping users navigate and understand repository structure and objectives.

General Structure and Usage:

All files are in Jupyter Notebook format and designed for hands-on learning, with stepwise code implementation and explanations.

Focuses on core data science practices: preprocessing, encoding, handling missing/skewed data, validating, and comparing ML models.

Each topic is modular—users can study encoding, transformation, imputation, validation, and model selection techniques independently or sequentially.

Especially helpful for students, early-career practitioners, or anyone looking to concretely understand and implement ML workflow best practices in Python.

This repository provides a practical journey through the main technical steps of supervised machine learning pipelines, from cleaning and preprocessing data to robust evaluation of models.​
