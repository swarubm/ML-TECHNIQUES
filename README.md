# SPECIALIZED ML-TECHNIQUES
CROSS VALIDATION .ipynb: Demonstrates four major cross-validation methods using the Iris dataset. Helps evaluate and compare model performance by partitioning data into training and testing segments, a core ML validation technique.

ROC-AUC curve.ipynb: Shows how to compute and plot ROC-AUC curves, which are used for judging binary classification model quality. This notebook includes code and practical outputs for interpreting classifier results.

encoding categorical data.ipynb: Provides a guide to encoding categorical variables using One-Hot Encoding and Label Encoding—essential for converting non-numeric data into ML-friendly formats.

imputation.ipynb: Covers various data imputation techniques as part of EDA (Exploratory Data Analysis), focusing on how to handle and fill missing values in datasets before modeling.

log transformation.ipynb: Teaches and demonstrates log transformation to convert skewed data distributions into more normal forms, improving model suitability and performance.

model_selection_and_comparing_with_classifiers.ipynb: Explores how to select models and compare different classifiers in practice. Includes code for fitting and evaluating various ML classifiers, possibly with comparative metrics and result analysis.

General Structure and Usage:

All files are in Jupyter Notebook format and designed for hands-on learning, with stepwise code implementation and explanations.

Focuses on core data science practices: preprocessing, encoding, handling missing/skewed data, validating, and comparing ML models.

Each topic is modular—users can study encoding, transformation, imputation, validation, and model selection techniques independently or sequentially.

Especially helpful for students, early-career practitioners, or anyone looking to concretely understand and implement ML workflow best practices in Python.

This repository provides a practical journey through the main technical steps of supervised machine learning pipelines, from cleaning and preprocessing data to robust evaluation of models.​
